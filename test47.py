# ã€Œtest43.pyã€ã®ãƒ¡ãƒ¢ã‚¢ãƒ—ãƒªã‚’ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰å½¢å¼ã¨ã—ã¦æœ€å°é™ã§æ§‹æˆ

import os
import csv
import json
import datetime
import argparse
import wcwidth
import re
from collections import Counter

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
NOTES_PATH = os.path.join(BASE_DIR, "data", "notes.json")

MAX_TITLE_LEN = 100
MAX_BODY_LEN = 1000

RED   = "\033[31m"
GREEN = "\033[32m"
YELLOW= "\033[33m"
BLUE  = "\033[34m"
HILITE = "\033[45m"
RESET = "\033[0m"

def visual_width(text: str) -> int:                 # wcwidthã§æ–‡å­—ã®è¡¨ã¯å¹…ã‚’å–å¾—ã—ã€åˆè¨ˆå¹…ã‚’æ±‚ã‚ã‚‹é–¢æ•°
    """æ–‡å­—åˆ—ã®è¦‹ãŸç›®ã®å¹…ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆå…¨è§’=2ã€åŠè§’=1ï¼‰"""
    return sum(wcwidth.wcwidth(ch) for ch in str(text or ""))

def pad(text: str, width: int) -> str:              # å¹…ã®å·®åˆ†ã ã‘ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¶³ã™é–¢æ•°
    """å·¦å¯„ã›ã§å¹…ã‚’æƒãˆã¾ã™ã€‚ï¼ˆå…¨è§’æ··ã˜ã‚Šã¯ã–ã£ãã‚Šã§OKï¼‰"""
    s = str(text or "")
    length = visual_width(s)
    padding = max(0, width - length)
    return s + " " * padding

def clip(text: str, width: int) -> str:             # å­—æ•°ã‚’ã‚ªãƒ¼ãƒãƒ¼ã™ã‚‹å ´åˆã¯ã€æŒ‡å®šã—ãŸæ–‡å­—æ•°ã§åˆ‡ã£ã¦ã€Œâ€¦ã€ã‚’ã¤ã‘ã‚‹é–¢æ•°
    """è¦‹ãŸç›®ã®å¹…ã§åˆ‡ã‚‹ï¼ˆå…¨è§’å¯¾å¿œï¼‰"""
    s = str(text or "")
    result = ""
    current_width = 0
    for ch in s:
        w = wcwidth.wcwidth(ch) or 0
        if current_width + w > width -1:
            result += "â€¦"
            break
        result += ch
        current_width += w
    return result

def highlight(text: str, words, case_sensitive: bool) -> str:   # è‰²ã‚³ãƒ¼ãƒ‰ã¯å¹…è¨ˆç®—ã«å½±éŸ¿ã™ã‚‹ã®ã§ã€clip/pad ã¯ãƒã‚¤ãƒ©ã‚¤ãƒˆã®å‰ã«ã‹ã‘ã‚‹ã®ãŒå®‰å…¨
    """text ä¸­ã® words ã‚’è‰²ä»˜ã‘ã€‚å¤§å°ç„¡è¦–ãªã‚‰å°æ–‡å­—åŒ–ã—ã¦æ¢ã™ã€‚"""
    s = str(text or "")
    if not words:
        return s
    
    escaped = [re.escape(w) for w in words if w] # æ­£è¦è¡¨ç¾ã§è¤‡æ•°èªã‚’ä¸€æ°—ã«ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆç‰¹æ®Šæ–‡å­—ã¯ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ï¼‰
    if not escaped:
        return s
    pattern = "|".join(escaped)
    flags = 0 if case_sensitive else re.IGNORECASE
    return re.sub(pattern, lambda m: f"{HILITE}{m.group(0)}{RESET}", s, flags=flags)

def error(msg, hint=None):
    print(f"âŒï¸ {msg}")
    if hint:
        print(f"å¯¾å‡¦: {hint}")

def validate_title(raw):
    title = (raw or "").strip()
    if title == "":
        error("ã‚¿ã‚¤ãƒˆãƒ«ã¯å¿…é ˆã§ã™ã€‚", "ç©ºç™½ä»¥å¤–ã®æ–‡å­—ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
        return None
    if "\n" in title:
        title = title.replace("\n", " ")
    if len(title) > MAX_TITLE_LEN:
        error(f"ã‚¿ã‚¤ãƒˆãƒ«ãŒé•·ã™ãã¾ã™ï¼ˆ{len(title)}ï¼‰æ–‡å­—", f"ä¸Šé™ã¯ {MAX_TITLE_LEN} æ–‡å­—ã§ã™ã€‚")
        return None
    return title

def validate_body(raw):
    if raw is None:
        return "(æœ¬æ–‡ãªã—)"
    body = str(raw).strip()
    if body == "":
        return "(æœ¬æ–‡ãªã—)"
    if len(body) > MAX_BODY_LEN:
        error(f"æœ¬æ–‡ãŒé•·ã™ãã¾ã™({len(body)}æ–‡å­—)", f"ä¸Šé™ã¯ {MAX_BODY_LEN} æ–‡å­—ã§ã™ã€‚")
        return None
    return body

# ===== ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿æ›¸ã =====
def load_notes(filepath):
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data or []
    except FileNotFoundError:
        return []
    except json.JSONDecodeError as e:
        error("JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒå£Šã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒã‚ã‚Œã°æˆ»ã™ã‹ã€æ‰‹ã§æ•´ãˆã¦ãã ã•ã„ã€‚")
        print(f"è©³ç´°: JSONDecodeError - {e}")
        return []
    except Exception as e:
        error("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¾ã—ãŸã€‚")
        print(f"è©³ç´°: {type(e).__name__} - {e}")
        return []
    
def save_notes(data, filepath):
    dirpath = os.path.dirname(filepath)
    os.makedirs(dirpath, exist_ok=True)
    tmp_path = os.path.join(dirpath, ".notes.json.tmp")
    try:
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        os.replace(tmp_path, filepath)
        print("âœ…ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except PermissionError as e:
        error("ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆæ¨©é™ä¸è¶³ï¼‰ã€‚", "data/ ãƒ•ã‚©ãƒ«ãƒ€ã‚„ notes.json ã®æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print(f"è©³ç´°: {type(e).__name__} - {e}")
    except Exception as e:
        error("ä¿å­˜å‡¦ç†ã§äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        print(f"è©³ç´°: {type(e).__name__} - {e}")
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

def next_id(data):
    if not data:
        return 1
    return max(row.get("id", 0) for row in data) + 1

# ===== ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã”ã¨ã®æœ¬ä½“ =====
def cmd_add(args):
    data = load_notes(NOTES_PATH)

    title = validate_title(args.title)
    if title is None:
        return
    body = validate_body(args.body)
    if body is None:
        return
    
    now = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
    note = {"id": next_id(data), "title": title, "body": body, "created_at": now}
    data.append(note)
    save_notes(data, NOTES_PATH)

def cmd_list(args):
    data = load_notes(NOTES_PATH)
    if not data:
        print("ä¸€è¦§è¡¨ç¤ºã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        print(f'{YELLOW}ã¾ãšã¯: python3 test47.py add "ã‚¿ã‚¤ãƒˆãƒ«" --body "æœ¬æ–‡"{RESET}')
        return
    print(f"===== ãƒ¡ãƒ¢ä¸€è¦§{len(data)}ä»¶ =====") 
    for row in data:
        created = row.get("created_at", "").replace("T", " ")[:16]
        id_col = f"[#{row.get('id')}]"
        title = clip(row.get("title", ""), 22)
        print(pad(id_col, 5), pad(title, 22), created)

def cmd_update(args):
    data = load_notes(NOTES_PATH)
    target_id = args.id

    if (args.title is None) and (args.body is None):
        error("å¤‰æ›´ã—ã¦ã„ãŒãªã„ãŸã‚ã€æ›´æ–°ã¯è¡Œã„ã¾ã›ã‚“ã§ã—ãŸã€‚", "--title ã¾ãŸã¯ --body ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        return
    
    found_index = None
    for i, row in enumerate(data):
        if row.get("id") == target_id:
            found_index = i
            break
    if found_index is None:
        error(f"è©²å½“ã®IDãŒã‚ã‚Šã¾ã›ã‚“: {target_id}", "ã¾ãš list ã§IDã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    
    current = data[found_index]

    if args.title is not None:
        checked = validate_title(args.title)
        if checked is None:
            return
        new_title = checked
    else:
        new_title = current.get("title", "")

    if args.body is not None:
        checked = validate_body(args.body)
        if checked is None:
            return
        new_body = checked
    else:
        new_body = current.get("body", "")

    current["title"] = new_title
    current["body"] = new_body
    current["updated_at"] = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

    save_notes(data, NOTES_PATH)

def cmd_delete(args):
    data = load_notes(NOTES_PATH)
    before = len(data)
    new_data = [row for row in data if row.get("id") != args.id]
    after = len(new_data)
    if before == after:
        error(f"è©²å½“ã®IDãŒã‚ã‚Šã¾ã›ã‚“: {args.id}", "list ã§å­˜åœ¨ã™ã‚‹IDã‚’ç¢ºèªã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return
    save_notes(new_data, NOTES_PATH)
    print(f"ğŸ—‘ï¸ å‰Šé™¤ã—ã¾ã—ãŸ(#{args.id})ã€‚ç¾åœ¨ã®ä»¶æ•°: {after}")

# ===== search ã‚³ãƒãƒ³ãƒ‰ã‚’è¿½åŠ  =====
def cmd_search(args):
    data = load_notes(NOTES_PATH)

    if not args.keywords:
        print(f"{RED}âŒï¸ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚{RESET}")
        return

    # å¤§æ–‡å­—å°æ–‡å­—ã®æ‰±ã„
    prep    = (lambda s: s or "")                                             # ã¾ãšã€Œãã®ã¾ã¾è¿”ã™ã€é–¢æ•°ã‚’å…¥ã‚Œã¦ãŠãï¼ˆå…¨çµŒè·¯ã§å­˜åœ¨ã•ã›ã‚‹ï¼‰
    kw_list = args.keywords[:]                                                # æ—¢å®šã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚ãã®ã¾ã¾ä½¿ã†

    if not args.case_sensitive:                                               # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ã¨ãã¯â€¦
        prep    = (lambda s: (s or "").lower())                               # å°æ–‡å­—åŒ–ã—ã¦ã‹ã‚‰æ¯”è¼ƒã™ã‚‹é–¢æ•°ã«ä¸Šæ›¸ã
        kw_list = [k.lower() for k in args.keywords]                          

    # æœŸé–“ã®æº–å‚™
    d_from = parse_date_ymd(args.date_from)
    d_to = parse_date_ymd(args.date_to)

    # ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶
    results = []
    for row in data:
        title = prep(row.get("title", ""))
        body = prep(row.get("body", ""))

        # å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é¸æŠ
        fields = []
        if args.scope == "title":
            fields = [title]
        elif args.scope == "body":
            fields = [body]
        else:
            fields = [title, body] # both

        # ---- ã“ã“ãŒè‚ï¼šè¤‡æ•°èª Ã— AND/OR ----
        # any: ã©ã‚Œã‹ã®èªãŒã©ã‚Œã‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å«ã¾ã‚Œã‚Œã°OK
        # all: ã™ã¹ã¦ã®èªãŒã€ã©ã‚Œã‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å«ã¾ã‚Œã‚‹å¿…è¦ãŒã‚ã‚‹
        def contains(word: str) -> bool:
            return any(word in f for f in fields)
        
        if args.match == "any":
            ok_text = any(contains(w) for w in kw_list)
        else:
            ok_text = all(contains(w) for w in kw_list)

        if not ok_text:
            continue

        # æ—¥ä»˜ç¯„å›²ï¼ˆcreated_atï¼‰
        created_at = row.get("created_at", "")
        d_created = to_dt(created_at)
        if d_from and (not d_created or d_created < d_from):
            continue
        if d_to:   # d_to ã®å½“æ—¥23:59:59ã¾ã§å«ã‚ãŸã„ã®ã§ã€ç¿Œæ—¥ã«é”ã—ãŸã‚‰é™¤å¤–
            edge = d_to.replace(hour=23, minute=59, second=59)
            if not d_created or d_created > edge:
                continue
        
        results.append(row)

    if not results:
        joined = " ".join(args.keywords)
        print(f"{RED}ã€Œ{joined}ã€ã‚’å«ã‚€ãƒ¡ãƒ¢ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚{RESET}")
        hints = []
        if args.scope != "both":
            hints.append(f"--in both ã‚’è©¦ã™")
        if not args.case_sensitive: 
            hints.append(f"--case-sensitive ã‚’è©¦ã™")
        hints.append("--match any/all ã®åˆ‡ã‚Šæ›¿ãˆã‚’è©¦ã™")
        if hints:
            print(f"{YELLOW}ãƒ’ãƒ³ãƒˆ:{RESET} " + " / ".join(hints))
        return
    
    # limit
    if args.limit and args.limit > 0:
        results = results[:args.limit]

    # è¦‹å‡ºã—ã¨ãƒ˜ãƒƒãƒ€
    joined = " ".join(args.keywords)
    print(f'{YELLOW}ğŸ” æ¤œç´¢çµæœ {len(results)} ä»¶{RESET}  '
          f'(scope={args.scope}, case={"æ•æ„Ÿ" if args.case_sensitive else "ç„¡è¦–"})')
    
    # ãƒ˜ãƒƒãƒ€è¡Œï¼ˆåˆ—å¹…ã‚’ãã‚ãˆã‚‹ï¼‰
    print(pad("ID", 6), pad("ã‚¿ã‚¤ãƒˆãƒ«", 24), "ä½œæˆæ—¥æ™‚")

    # æœ¬æ–‡
    for row in results:
        created = row.get("created_at", "").replace("T", " ")[:16]
        id_col = f"[#{row.get('id')}]"
        title_raw = row.get("title", "")
        title_shr = clip(title_raw, 24)
        title_out = highlight(title_shr, args.keywords, args.case_sensitive)
        print(pad(id_col, 6), pad(title_out, 24), created)

    # --- ã‚‚ã— --export ãŒæŒ‡å®šã•ã‚Œã¦ã„ãŸã‚‰æ›¸ãå‡ºã™ ---------
    if getattr(args, "export", None):
        export_results(results, args.export)

    # --- ã‚‚ã— --stats ãŒæŒ‡å®šã•ã‚Œã¦ã„ãŸã‚‰ç°¡æ˜“é›†è¨ˆã‚’è¡¨ç¤º -----
    if getattr(args, "stats", False):
        summarize_results(results, by=getattr(args, "by", "date"),
                          limit=getattr(args, "limit_stats", 10))

def parse_date_ymd(s: str): # æ—¥ä»˜ã§æ¤œç´¢ã™ã‚‹ãŸã‚ã®é–¢æ•°ï¼ˆäººé–“æ–‡å­—ã‹ã‚‰PCæ–‡å­—ã«å¤‰æ›ï¼‰
    """YYYY-MM-DD ã‚’ datetime ã«ã€‚ç©ºã‚„ä¸æ­£ã¯ None ã‚’è¿”ã™ã€‚"""
    if not s:
        return None
    try:
        return datetime.datetime.strptime(s.strip(), "%Y-%m-%d")
    except Exception:
        return None
    
def to_dt(created_at: str): # æ™‚åˆ»ã§æ¤œç´¢ã™ã‚‹ãŸã‚ã®é–¢æ•°ï¼ˆäººé–“æ–‡å­—ã‹ã‚‰PCæ–‡å­—ã«å¤‰æ›ï¼‰
    """created_at(YYYY-MM-DDTHH:MM:SS) ã‚’ datetime ã«ã€‚å¤±æ•—ã¯ Noneã€‚"""
    try:
        return datetime.datetime.strptime((created_at or "")[:19], "%Y-%m-%dT%H:%M:%S")
    except Exception:
        return None
    
def summarize_results(results, by="date", limit=10):    # æ¤œç´¢çµæœã‚’ã‚¹ãƒ”ãƒ¼ãƒ‡ã‚£ã«è¦ç´„ã—ã¦ä½¿ã„æ‰€ã‚’å¢—ã‚„ã™é–¢æ•°
    """æ¤œç´¢çµæœã‚’ç°¡æ˜“é›†è¨ˆã—ã¦è¡¨ç¤ºã™ã‚‹ï¼ˆby=date/titleï¼‰"""
    # å…¨ä½“ã‚µãƒãƒª
    total = len(results)
    dates = []
    title_lens = []
    empty_body = 0

    for row in results:
        created = (row.get("created_at", "")[:10] or "")
        if created:
            dates.append(created)
        t = row.get("title", "")
        title_lens.append(len(str(t)))
        if (row.get("body", "") or "") in ("", "(æœ¬æ–‡ãªã—)"):
            empty_body += 1
    
    date_min = min(dates) if dates else "-"
    date_max = max(dates) if dates else "-"
    avg_title = (sum(title_lens)/len(title_lens)) if title_lens else 0.0

    # è¦‹å‡ºã—ï¼ˆå…¨ä½“ã‚µãƒãƒªï¼‰
    print(f"\n{BLUE}ğŸ“Š é›†è¨ˆã‚µãƒãƒª{RESET}")
    print(f"  åˆè¨ˆ: {total} ä»¶")
    print(f"  æœŸé–“: {date_min} ã€œ {date_max}")
    print(f"  å¹³å‡ã‚¿ã‚¤ãƒˆãƒ«é•·: {avg_title:.1f} æ–‡å­—")
    print(f"  æœ¬æ–‡ãªã—: {empty_body} ä»¶")

    # è»¸åˆ¥ã®å†…è¨³
    key_list = []
    if by == "date":
        for row in results:
            key_list.append((row.get("created_at", "")[:10] or "ä¸æ˜æ—¥ä»˜"))
        label = "æ—¥ä»˜"
    else:
        for row in results:
            key_list.append((row.get("title", "") or "(ç„¡é¡Œ)"))
        label = "ã‚¿ã‚¤ãƒˆãƒ«"

    counter = Counter(key_list)
    ranked = sorted(counter.items(), key=lambda kv: (-kv[1], str(kv[0])))   # ä»¶æ•°ã®å¤šã„é † â†’ ã‚­ãƒ¼åé †ã®è¤‡åˆã‚½ãƒ¼ãƒˆã§è¦‹ã‚„ã™ã

    if limit and limit > 0:
        ranked = ranked[:limit]

    # è¡¨æ•¬å¼ã§å‡ºåŠ›
    print(f"\n{BLUE}ğŸ“” å†…è¨³(by={by}){RESET}")
    print(pad(label, 26), pad("ä»¶æ•°", 6))
    for k, cnt in ranked:
        k_shr = clip(str(k), 26)
        print(pad(k_shr, 26), pad(str(cnt), 6))
    
def export_results(results, mode="csv"):    # æ¤œç´¢çµæœã‚’CSVã‚„JSONã«æ›¸ãå‡ºã™é–¢æ•°ã‚’è¿½åŠ 
    """æ¤œç´¢çµæœã‚’CSVã¾ãŸã¯JSONã«ä¿å­˜ã™ã‚‹ï¼ˆåŒåå›é¿ã®ãŸã‚æ™‚åˆ»ã§ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œã‚‹ï¼‰"""
    now = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    filename = f"export_{now}.{mode}"

    # ---- åˆ—ï¼ˆã‚­ãƒ¼ï¼‰ã‚’ãã‚ãˆã‚‹ä¸‹ã”ã—ã‚‰ãˆ -----------------------------------
    # ã©ã†ã—ã¦ï¼Ÿ â†’ ãƒ¬ã‚³ãƒ¼ãƒ‰ã”ã¨ã«æŒã¤ã‚­ãƒ¼ãŒå¾®å¦™ã«é•ã£ã¦ã‚‚ã€CSVã®åˆ—ãŒå´©ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚
    all_keys = set()
    for row in results:
        all_keys.update(row.keys())
    fieldnames = sorted(all_keys)

    try:
        if mode == "csv":
            with open(filename, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for row in results:
                    safe_row = {key: row.get(key, "") for key in fieldnames}
                    writer.writerow(safe_row)
        else:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
         
        print(f"{GREEN}âœ…ï¸ æ¤œç´¢çµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚{RESET}")
    except Exception as e:
        print(f"{RED}âŒï¸ æ›¸ãå‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ:{RESET} {type(e).__name__} - {e}")

# ===== å¼•æ•°ï¼ˆã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ï¼‰ã®å®šç¾© =====
def parse_args():
    parser = argparse.ArgumentParser(
        description="JSONãƒ¡ãƒ¢ã‚¢ãƒ—ãƒªï¼ˆã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ç‰ˆï¼‰\nadd / list / updata / delete ã‚’ä½¿ã£ã¦æ“ä½œã§ãã¾ã™ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    subparsers = parser.add_subparsers(dest="command", help="åˆ©ç”¨ã§ãã‚‹ã‚³ãƒãƒ³ãƒ‰")

    # add
    p_add = subparsers.add_parser("add", help="ãƒ¡ãƒ¢ã‚’è¿½åŠ ")
    p_add.add_argument("title", help="ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŒ‡å®š")
    p_add.add_argument("--body", help="æœ¬æ–‡ã‚’æŒ‡å®š", default="ï¼ˆæœ¬æ–‡ãªã—ï¼‰")
    p_add.set_defaults(func=cmd_add)

    # list
    p_list = subparsers.add_parser("list", help="ãƒ¡ãƒ¢ä¸€è¦§ã‚’è¡¨ç¤º")
    p_list.set_defaults(func=cmd_list)

    # update
    p_upd = subparsers.add_parser("update", help="ãƒ¡ãƒ¢ã‚’æ›´æ–°")
    p_upd.add_argument("id", type=int, help="æ›´æ–°å¯¾è±¡ã®ID")
    p_upd.add_argument("--title", help="æ–°ã—ã„ã‚¿ã‚¤ãƒˆãƒ«")
    p_upd.add_argument("--body", help="æ–°ã—ã„æœ¬æ–‡")
    p_upd.set_defaults(func=cmd_update)

    # delete
    p_del = subparsers.add_parser("delete", help="ãƒ¡ãƒ¢ã‚’å‰Šé™¤")
    p_del.add_argument("id", type=int, help="å‰Šé™¤å¯¾è±¡ã®ID")
    p_del.set_defaults(func=cmd_delete)

    # search
    p_search = subparsers.add_parser("search", help="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ¡ãƒ¢ã‚’æ¤œç´¢")
    p_search.add_argument("keywords", nargs="+", help="æ¤œç´¢ã—ãŸã„æ–‡å­—åˆ—ã‚’æŒ‡å®š")  # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§è¤‡æ•°æŒ‡å®šOK
    p_search.add_argument("--match", choices=["any", "all"], default="any",
                          help="any=ã©ã‚Œã‹å«ã‚€ï¼ˆORï¼‰/ all=ã™ã¹ã¦å«ã‚€ï¼ˆANDï¼‰")       # AND/ORã®åˆ‡ã‚Šæ›¿ãˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
    p_search.add_argument("--in", dest="scope", choices=["title", "body", "both"],
                          default="both", help="æ¤œç´¢å¯¾è±¡ï¼ˆtitle/body/bothï¼‰")
    p_search.add_argument("--from", dest="date_from", help="é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDï¼‰")
    p_search.add_argument("--to", dest="date_to", help="çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDï¼‰")
    p_search.add_argument("--case-sensitive", action="store_true", help="å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥")
    p_search.add_argument("--limit", type=int, default=0, help="æœ€å¤§è¡¨ç¤ºä»¶æ•°ï¼ˆ0ã¯åˆ¶é™ãªã—ï¼‰")

    p_search.add_argument("--stats", action="store_true", help="æ¤œç´¢çµæœã‚’è¡¨ç¤ºã™ã‚‹")
    p_search.add_argument("--by", choices=["date", "title"], default="date", help="é›†è¨ˆã®è»¸ï¼ˆdate=ä½œæˆæ—¥ã”ã¨ / title=ã‚¿ã‚¤ãƒˆãƒ«ã”ã¨ï¼‰")
    p_search.add_argument("--limit-stats", type=int, default=10, help="é›†è¨ˆè¡¨ç¤ºã®æœ€å¤§è¡Œæ•°ï¼ˆ0ã¯åˆ¶é™ãªã—ï¼‰")

    # export
    p_search.add_argument("--export", choices=["csv", "json"], help="æ¤œç´¢çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆcsv/jsonï¼‰")
    p_search.set_defaults(func=cmd_search)

    

    return parser.parse_args()

def main():
    args = parse_args()
    if hasattr(args, "func"):
        args.func(args)
    else:
        print("â—ã‚³ãƒãƒ³ãƒ‰ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆadd / list / update / delete / searchï¼‰")

if __name__ == "__main__":
    main()